# Watchtower Phase 1 Implementation Guide

**Duration**: 4-6 weeks  
**Goal**: Basic MITRE + Process correlation foundation  
**Team Size**: 2-3 developers + 1 security analyst  

## üìã **Phase 1 Overview**

Phase 1 establishes the foundational multi-agent architecture for Watchtower with basic MITRE ATT&CK correlation and process analysis capabilities. This MVP validates the core agent framework and provides immediate value through automated threat technique identification.

### **Success Criteria**
- ‚úÖ Read MITRE ATT&CK techniques from JSON
- ‚úÖ Process basic process lists and artifacts
- ‚úÖ Create simple suspicious activity timeline
- ‚úÖ Map 2-3 common techniques (T1003, T1059, T1083)
- ‚úÖ Generate basic incident reports

## üèóÔ∏è **Architecture Implementation**

### **Agent Structure for Phase 1**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 BASIC ORCHESTRATOR                          ‚îÇ
‚îÇ              (Sequential Pattern Only)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ              PHASE 1 AGENTS                  ‚îÇ
        ‚îÇ          (Sequential Execution)              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº             ‚ñº               ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MITRE     ‚îÇ ‚îÇ  PROCESS    ‚îÇ ‚îÇ  TIMELINE   ‚îÇ ‚îÇ   REPORT    ‚îÇ
‚îÇ   AGENT     ‚îÇ ‚îÇ   AGENT     ‚îÇ ‚îÇ   AGENT     ‚îÇ ‚îÇ   AGENT     ‚îÇ
‚îÇ             ‚îÇ ‚îÇ             ‚îÇ ‚îÇ             ‚îÇ ‚îÇ             ‚îÇ
‚îÇ ‚Ä¢ ATT&CK    ‚îÇ ‚îÇ ‚Ä¢ Process   ‚îÇ ‚îÇ ‚Ä¢ Basic     ‚îÇ ‚îÇ ‚Ä¢ JSON      ‚îÇ
‚îÇ   Loading   ‚îÇ ‚îÇ   Lists     ‚îÇ ‚îÇ   Mapping   ‚îÇ ‚îÇ   Output    ‚îÇ
‚îÇ ‚Ä¢ TTP       ‚îÇ ‚îÇ ‚Ä¢ CMD Lines ‚îÇ ‚îÇ ‚Ä¢ Timeline  ‚îÇ ‚îÇ ‚Ä¢ Simple    ‚îÇ
‚îÇ   Mapping   ‚îÇ ‚îÇ ‚Ä¢ Users     ‚îÇ ‚îÇ   Creation  ‚îÇ ‚îÇ   Report    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß **Technology Stack**

### **Core Framework**
```python
# Primary Dependencies (already in pyproject.toml)
azure-ai-agents = ">=1.0.0"         # Azure AI Foundry agents framework
azure-ai-projects = ">=1.0.0b11"    # Azure AI project management
azure-identity = ">=1.19.0"         # Already included - Azure authentication
aiohttp = ">=3.12.15"               # Already included - HTTP client
python-dotenv = ">=1.1.1"           # Already included - Environment config

# Additional Dependencies for Phase 1
semantic-kernel = ">=1.15.0"        # Optional: Enhanced orchestration patterns
pytest-asyncio = ">=0.24.0"         # Async testing
httpx = ">=0.28.0"                  # HTTP client for testing
```

### **Azure AI Foundry Models (Already Deployed)**
Based on your existing infrastructure:

**Primary**: `gpt-4o-mini` (Azure OpenAI - Already Deployed)
- **Deployment**: GlobalStandard SKU, 30 capacity
- **Version**: 2024-07-18
- **Cost**: Optimized for development and production
- **Context**: 128K tokens (sufficient for forensic analysis)
- **Connection**: Azure AI Foundry native integration

**Embedding Model**: `text-embedding-3-small` (Optional for Phase 1)
- **Dimensions**: 100 (configurable)
- **Use Case**: Future semantic search capabilities

### **Existing Azure Infrastructure**
Your deployed infrastructure includes:
```yaml
‚úÖ Azure AI Foundry Hub & Project
‚úÖ Azure OpenAI Service (gpt-4o-mini)
‚úÖ Azure AI Search Service (optional for Phase 1)
‚úÖ Application Insights (monitoring)
‚úÖ Azure Storage Account (data persistence)
‚úÖ Log Analytics Workspace (logging)
```

## üìÅ **Project Structure**

```
src/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py          # Abstract base class
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent_config.py        # Configuration classes
‚îÇ   ‚îú‚îÄ‚îÄ mitre/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mitre_agent.py         # MITRE ATT&CK analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mitre_data.py          # MITRE data models
‚îÇ   ‚îú‚îÄ‚îÄ forensic/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_agent.py       # Process analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ forensic_models.py     # Forensic data models
‚îÇ   ‚îú‚îÄ‚îÄ timeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ timeline_agent.py      # Timeline synthesis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ timeline_models.py     # Timeline data models
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ basic_orchestrator.py  # Phase 1 orchestrator
‚îÇ       ‚îî‚îÄ‚îÄ orchestration_models.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ mitre/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enterprise-attack.json # MITRE ATT&CK data
‚îÇ   ‚îî‚îÄ‚îÄ test_data/
‚îÇ       ‚îú‚îÄ‚îÄ process_samples.json   # Test process data
‚îÇ       ‚îî‚îÄ‚îÄ expected_outputs.json  # Expected results
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent_models.py           # Shared data models
‚îÇ   ‚îú‚îÄ‚îÄ mitre_models.py           # MITRE-specific models
‚îÇ   ‚îî‚îÄ‚îÄ forensic_models.py        # Forensic data models
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py         # Logging setup
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py             # Custom exceptions
‚îî‚îÄ‚îÄ main.py                       # Entry point
```

## üè≠ **Azure Infrastructure Deployment**

### **Prerequisites (Already Completed)**
Your existing infrastructure is ready! You have:

```bash
# Deploy your Azure AI Foundry infrastructure (if not already done)
azd up

# This deploys:
# ‚úÖ Azure AI Foundry Hub & Project
# ‚úÖ Azure OpenAI Service (gpt-4o-mini)  
# ‚úÖ Azure AI Search Service
# ‚úÖ Application Insights
# ‚úÖ Azure Storage Account
# ‚úÖ Log Analytics Workspace
```

### **Environment Setup**
After deployment, your `.env` file will be automatically populated by the post-deploy scripts:

```bash
# These values are set by ./scripts/write_env.sh
AZURE_CLIENT_ID=xxxxxxxxxxxxxxxxxxxx
AZURE_TENANT_ID=xxxxxxxxxxxxxxxxxxxx  
AZURE_SUBSCRIPTION_ID=xxxxxxxxxxxxxxxxxxxx
AZUREAI_PROJECT_CONNECTION_STRING=xxxxxxxx
AZUREAI_PROJECT_NAME=xxxxxxxxxxxxxxxx

# Agent-specific configuration
AGENT_DEPLOYMENT_NAME=gpt-4o-mini        # Your deployed model
EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small
AZURE_RESOURCE_GROUP=rg-watchtower-xxx   # Your resource group
```

### **Connecting to Your Deployed Resources**
The Phase 1 agents will use your existing Azure AI Foundry resources:

1. **Azure AI Foundry Project**: Central hub for agent coordination
2. **gpt-4o-mini Deployment**: Primary model for all agents
3. **Application Insights**: Automatic monitoring and telemetry
4. **Azure Storage**: Artifact persistence and caching
```

## üöÄ **Implementation Roadmap**

### **Week 1: Foundation Setup**

#### **Day 1-2: Infrastructure Validation & Setup**

**Tasks:**
1. **Validate Azure Deployment**
   ```bash
   # Ensure your Azure infrastructure is deployed
   azd status
   
   # If not deployed, run:
   azd up
   
   # Verify environment variables are set
   source .env && echo $AZUREAI_PROJECT_NAME
   ```

2. **Test Azure AI Foundry Connection**
   ```python
   # Quick connection test script
   from azure.ai.projects import AzureAIProjectsClient
   from azure.identity import DefaultAzureCredential
   
   credential = DefaultAzureCredential()
   client = AzureAIProjectsClient.from_connection_string(
       connection_string=os.getenv("AZUREAI_PROJECT_CONNECTION_STRING")
   )
   
   # Verify deployment access
   deployments = client.deployments.list()
   print(f"Available deployments: {[d.name for d in deployments]}")
   ```

3. **Update Dependencies**
   ```toml
   # Add to pyproject.toml [dependencies] (if not already present)
   semantic-kernel = ">=1.15.0"  # Optional: Enhanced orchestration
   
   # Add to [dependency-groups.dev]
   pytest-asyncio = ">=0.24.0"
   httpx = ">=0.28.0"
   
   # Already included for Azure AI Foundry:
   # azure-ai-agents = ">=1.0.0"
   # azure-ai-projects = ">=1.0.0b11" 
   # azure-identity = ">=1.19.0"
   ```

2. **Environment Configuration**
   ```bash
   # .env file (generated by azd postdeploy scripts)
   AZURE_CLIENT_ID=xxxxxxxxxxxxxxxxxxxx      # From Azure deployment
   AZURE_TENANT_ID=xxxxxxxxxxxxxxxxxxxx      # From Azure deployment  
   AZURE_SUBSCRIPTION_ID=xxxxxxxxxxxxxxxxxxxx # From Azure deployment
   AZUREAI_PROJECT_CONNECTION_STRING=xxxxxxxx # From Azure AI Foundry
   AZUREAI_PROJECT_NAME=xxxxxxxxxxxxxxxx      # From deployment
   LOG_LEVEL=INFO
   MITRE_DATA_PATH=./src/data/mitre/enterprise-attack.json
   
   # Optional: For local development
   AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
   AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini
   ```

3. **Base Infrastructure**
   - Create project structure
   - Set up logging configuration
   - Implement configuration management
   - Create base agent abstract class

#### **Day 3-5: Core Models and Utilities**

**Tasks:**
1. **Data Models Implementation**
   ```python
   # Key models to implement:
   - MitreTechnique
   - ProcessArtifact  
   - TimelineEvent
   - AnalysisResult
   ```

2. **Utility Functions**
   - Logging configuration
   - Configuration loader
   - Custom exception classes
   - Data validation utilities

3. **MITRE Data Setup**
   - Download latest MITRE ATT&CK JSON
   - Create data loading utilities
   - Implement MITRE data parser

### **Week 2: Agent Implementation**

#### **Day 6-8: MITRE Agent Development**

**Responsibilities:**
- Load and parse MITRE ATT&CK framework
- Identify relevant techniques from process data
- Map process indicators to MITRE techniques
- Provide technique confidence scoring

**Key Implementation:**
```python
class MitreAgent(BaseAgent):
    async def analyze_artifacts(self, artifacts: List[ProcessArtifact]) -> List[MitreTechnique]:
        # Implementation focuses on:
        # - T1003: OS Credential Dumping
        # - T1059: Command and Scripting Interpreter  
        # - T1083: File and Directory Discovery
```

#### **Day 9-10: Process Agent Development**

**Responsibilities:**
- Parse process lists and command lines
- Extract suspicious process indicators
- Identify process relationships
- Generate process-based IOCs

**Key Implementation:**
```python
class ProcessAgent(BaseAgent):
    async def analyze_processes(self, process_data: str) -> List[ProcessArtifact]:
        # Focus on:
        # - Command line analysis
        # - Process ancestry
        # - Suspicious process names
        # - Encoded commands
```

### **Week 3: Timeline and Orchestration**

#### **Day 11-13: Timeline Agent Development**

**Responsibilities:**
- Correlate MITRE techniques with process events
- Create chronological timeline
- Calculate confidence scores
- Generate analysis summaries

**Key Implementation:**
```python
class TimelineAgent(BaseAgent):
    async def create_timeline(self, mitre_results: List[MitreTechnique], 
                            process_results: List[ProcessArtifact]) -> Timeline:
        # Implementation focuses on:
        # - Event correlation
        # - Timeline ordering
        # - Confidence calculation
```

#### **Day 14-15: Basic Orchestrator**

**Responsibilities:**
- Coordinate agent execution
- Manage data flow between agents
- Handle error cases and retries
- Provide execution monitoring

**Key Implementation:**
```python
class BasicOrchestrator:
    async def analyze_incident(self, input_data: Dict) -> AnalysisResult:
        # Sequential execution:
        # 1. MITRE Agent loads framework
        # 2. Process Agent analyzes artifacts  
        # 3. Timeline Agent correlates results
        # 4. Generate final report
```

### **Week 4: Integration and Testing**

#### **Day 16-18: Integration Testing**

**Tasks:**
1. **End-to-End Integration**
   - Complete agent integration
   - Test full workflow execution
   - Validate output formats

2. **Test Data Creation**
   - Create realistic process samples
   - Generate expected outputs
   - Build test scenarios for common techniques

3. **Error Handling**
   - Implement retry mechanisms
   - Add timeout handling
   - Create fallback behaviors

#### **Day 19-21: Validation and Optimization**

**Tasks:**
1. **Accuracy Validation**
   - Test against known attack samples
   - Validate MITRE technique mapping
   - Measure false positive/negative rates

2. **Performance Optimization**
   - Optimize LLM prompt efficiency
   - Implement basic caching
   - Reduce API call overhead

3. **Documentation**
   - API documentation
   - Usage examples
   - Troubleshooting guide

### **Weeks 5-6: Polish and Deployment Preparation**

#### **Day 22-28: Production Readiness**

**Tasks:**
1. **Comprehensive Testing**
   - Unit tests for all agents
   - Integration test suite
   - Load testing for scalability

2. **Security Implementation**
   - Input validation and sanitization
   - Secure credential management
   - Audit logging implementation

3. **Monitoring and Observability**
   - Performance metrics collection
   - Error tracking and alerting
   - Usage analytics

#### **Day 29-35: Final Validation**

**Tasks:**
1. **User Acceptance Testing**
   - Security analyst validation
   - Real-world scenario testing
   - Feedback incorporation

2. **Deployment Pipeline**
   - CI/CD pipeline setup
   - Environment configuration
   - Deployment automation

3. **Handover Preparation**
   - Operational documentation
   - Support procedures
   - Phase 2 planning

## üíª **Key Implementation Details**

### **1. MITRE Agent Implementation**

```python
# src/agents/mitre/mitre_agent.py
from azure.ai.agents import AzureAIAgent
from azure.ai.projects import AzureAIProjectsClient
from azure.identity import DefaultAzureCredential

class MitreAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        # Use Azure AI Foundry native integration
        self.credential = DefaultAzureCredential()
        self.client = AzureAIProjectsClient(
            credential=self.credential,
            subscription_id=config.subscription_id,
            resource_group_name=config.resource_group,
            project_name=config.project_name
        )
        
        # Create agent using your deployed gpt-4o-mini model
        self.agent = AzureAIAgent(
            client=self.client,
            name="MITRE_Intelligence_Agent",
            description="MITRE ATT&CK framework analysis expert",
            instructions=self._get_mitre_instructions(),
            model_deployment_name="gpt-4o-mini"  # Your deployed model
        )
    
    def _get_mitre_instructions(self) -> str:
        return """
        You are a MITRE ATT&CK framework expert. Analyze process artifacts and identify 
        relevant MITRE techniques. Focus on:
        
        1. T1003 - OS Credential Dumping: Look for credential access tools
        2. T1059 - Command and Scripting Interpreter: Identify suspicious scripts
        3. T1083 - File and Directory Discovery: Detect enumeration activities
        
        Provide confidence scores (0.0-1.0) for each technique identification.
        Output format: JSON with technique_id, confidence, evidence, description.
        """
    
    async def analyze_artifacts(self, artifacts: List[ProcessArtifact]) -> List[MitreTechnique]:
        # Process artifacts through LLM analysis
        # Return structured MITRE technique mappings
```

### **2. Process Agent Implementation**

```python
# src/agents/forensic/process_agent.py
class ProcessAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        # Use Azure AI Foundry client
        self.credential = DefaultAzureCredential()
        self.client = AzureAIProjectsClient(
            credential=self.credential,
            subscription_id=config.subscription_id,
            resource_group_name=config.resource_group,
            project_name=config.project_name
        )
        
        self.agent = AzureAIAgent(
            client=self.client,
            name="Process_Analysis_Agent",
            description="Digital forensic process analysis expert",
            instructions=self._get_process_instructions(),
            model_deployment_name="gpt-4o-mini"  # Your deployed model
        )
        
        self.suspicious_patterns = [
            r"powershell.*-enc.*",  # Encoded PowerShell
            r"cmd\.exe.*&.*",       # Command chaining
            r".*\.(bat|cmd|ps1).*", # Script execution
        ]
    
    async def analyze_processes(self, process_data: str) -> List[ProcessArtifact]:
        # Parse process data and identify suspicious patterns
        # Extract command lines, users, timestamps
        # Return structured process artifacts
```

### **3. Timeline Agent Implementation**

```python
# src/agents/timeline/timeline_agent.py
class TimelineAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        self.credential = DefaultAzureCredential()
        self.client = AzureAIProjectsClient(
            credential=self.credential,
            subscription_id=config.subscription_id,
            resource_group_name=config.resource_group,
            project_name=config.project_name
        )
        
        self.agent = AzureAIAgent(
            client=self.client,
            name="Timeline_Synthesis_Agent", 
            description="Attack timeline correlation and mapping expert",
            instructions=self._get_timeline_instructions(),
            model_deployment_name="gpt-4o-mini"  # Your deployed model
        )
    
    async def create_timeline(self, mitre_results: List[MitreTechnique], 
                            process_results: List[ProcessArtifact]) -> Timeline:
        # Correlate MITRE techniques with process events
        # Create chronological sequence
        # Calculate overall confidence scores
        # Generate narrative summary
```

### **4. Basic Orchestrator Implementation**

```python
# src/agents/orchestrator/basic_orchestrator.py
from azure.ai.projects import AzureAIProjectsClient
from azure.identity import DefaultAzureCredential

class BasicOrchestrator:
    def __init__(self, config: OrchestratorConfig):
        # Single Azure AI Foundry client for coordination
        self.credential = DefaultAzureCredential()
        self.client = AzureAIProjectsClient(
            credential=self.credential,
            subscription_id=config.subscription_id,
            resource_group_name=config.resource_group,
            project_name=config.project_name
        )
        
        # Initialize agents with shared client
        self.mitre_agent = MitreAgent(config.mitre_config)
        self.process_agent = ProcessAgent(config.process_config)
        self.timeline_agent = TimelineAgent(config.timeline_config)
    
    async def analyze_incident(self, input_data: Dict) -> AnalysisResult:
        try:
            # Step 1: Initialize MITRE framework
            await self.mitre_agent.load_framework()
            
            # Step 2: Analyze process data
            process_artifacts = await self.process_agent.analyze_processes(
                input_data["process_data"]
            )
            
            # Step 3: Map to MITRE techniques
            mitre_results = await self.mitre_agent.analyze_artifacts(process_artifacts)
            
            # Step 4: Create timeline
            timeline = await self.timeline_agent.create_timeline(
                mitre_results, process_artifacts
            )
            
            # Step 5: Generate final report
            return AnalysisResult(
                timeline=timeline,
                mitre_techniques=mitre_results,
                process_artifacts=process_artifacts,
                confidence_score=self._calculate_overall_confidence(timeline),
                generated_at=datetime.utcnow()
            )
            
        except Exception as e:
            # Handle errors with fallback mechanisms
            return self._generate_error_result(e)
```

## üß™ **Testing Strategy**

### **Unit Testing**
```python
# tests/agents/test_mitre_agent.py
@pytest.mark.asyncio
async def test_mitre_agent_technique_identification():
    agent = MitreAgent(test_config)
    artifacts = [ProcessArtifact(
        process_name="powershell.exe",
        command_line="powershell -enc Base64EncodedCommand",
        user="SYSTEM"
    )]
    
    results = await agent.analyze_artifacts(artifacts)
    
    assert len(results) > 0
    assert any(r.technique_id == "T1059" for r in results)
    assert all(0.0 <= r.confidence <= 1.0 for r in results)
```

### **Integration Testing**
```python
# tests/integration/test_basic_orchestrator.py
@pytest.mark.asyncio 
async def test_end_to_end_analysis():
    orchestrator = BasicOrchestrator(test_config)
    
    input_data = {
        "process_data": load_test_process_data("credential_dump_scenario.json")
    }
    
    result = await orchestrator.analyze_incident(input_data)
    
    assert result.confidence_score > 0.7
    assert "T1003" in [t.technique_id for t in result.mitre_techniques]
    assert len(result.timeline.events) > 0
```

## üìä **Success Metrics and Validation**

### **Accuracy Targets**
- **MITRE Technique Identification**: 80% accuracy on common techniques (T1003, T1059, T1083)
- **Process Analysis**: 85% accuracy in suspicious process detection
- **Timeline Generation**: 90% correct chronological ordering
- **False Positive Rate**: <15% for high-confidence results

### **Performance Targets**
- **End-to-End Analysis**: <60 seconds for typical incident
- **Agent Response Time**: <10 seconds per agent (using deployed gpt-4o-mini)
- **Memory Usage**: <500MB for standard analysis
- **Azure AI Costs**: Predictable pricing through deployed models (no per-API-call surprises)
- **Deployment Utilization**: Efficient use of your 30-capacity gpt-4o-mini deployment

### **Validation Approach**

1. **Test Dataset Creation**
   ```json
   {
     "test_scenarios": [
       {
         "name": "credential_dumping_mimikatz",
         "process_data": "...",
         "expected_techniques": ["T1003.001"],
         "expected_confidence": "> 0.8"
       },
       {
         "name": "powershell_execution", 
         "process_data": "...",
         "expected_techniques": ["T1059.001"],
         "expected_confidence": "> 0.7"
       }
     ]
   }
   ```

2. **Automated Validation Pipeline**
   - Run test scenarios daily
   - Compare results against expected outputs  
   - Track accuracy metrics over time
   - Alert on regression

## üö® **Risk Mitigation**

### **Technical Risks**

1. **LLM Consistency Issues**
   - **Risk**: Inconsistent analysis results across runs
   - **Mitigation**: Implement result caching, use temperature=0 for deterministic outputs
   - **Monitoring**: Track result variance across multiple runs

2. **API Rate Limiting** 
   - **Risk**: GitHub Models rate limits affect performance
   - **Mitigation**: Implement exponential backoff, queue management, fallback to OpenAI
   - **Monitoring**: Track API usage and response times

3. **False Positive/Negative Rates**
   - **Risk**: Inaccurate threat detection affects analyst trust
   - **Mitigation**: Conservative confidence thresholds, human validation loop
   - **Monitoring**: Analyst feedback tracking and model retraining

### **Implementation Risks**

1. **Scope Creep**
   - **Risk**: Adding complexity beyond Phase 1 goals
   - **Mitigation**: Strict feature freeze, defer enhancements to Phase 2
   - **Monitoring**: Weekly scope review meetings

2. **Integration Complexity**
   - **Risk**: Agent integration issues delay delivery  
   - **Mitigation**: Early integration testing, modular design
   - **Monitoring**: Daily integration test runs

## üìà **Phase 1 Deliverables**

### **Code Deliverables**
- [ ] Complete multi-agent framework implementation
- [ ] MITRE ATT&CK integration with 3 core techniques
- [ ] Process analysis engine with pattern detection
- [ ] Timeline synthesis and correlation engine
- [ ] Basic orchestrator with sequential execution
- [ ] Comprehensive test suite (>80% coverage)
- [ ] API documentation and usage examples

### **Documentation Deliverables**
- [ ] Technical architecture documentation
- [ ] API reference documentation  
- [ ] Deployment and configuration guide
- [ ] Troubleshooting and FAQ guide
- [ ] Phase 2 planning document

### **Validation Deliverables**
- [ ] Test scenario library with expected results
- [ ] Accuracy validation report
- [ ] Performance benchmarking results
- [ ] Security assessment report
- [ ] User acceptance test results

## ‚û°Ô∏è **Phase 2 Preparation**

### **Technical Foundation for IOC Integration**
- Agent communication interfaces designed for async operations
- Modular architecture supports adding IOC reputation agent
- Caching infrastructure ready for external API results (Azure Storage)
- Error handling framework supports API failures

### **Data Pipeline Extensions**
- Process artifact models extensible for network/registry data
- Timeline engine supports multiple artifact types
- MITRE agent ready for sub-technique analysis
- Azure AI Foundry supports seamless model scaling for additional agents

### **Built-in Observability with Azure AI Foundry**
Your Phase 1 implementation automatically benefits from:

- **Application Insights**: Agent performance monitoring and telemetry
- **Azure Monitor**: Resource utilization and scaling metrics
- **Log Analytics**: Centralized logging and debugging
- **Cost Management**: Built-in Azure cost tracking and optimization
- **Model Metrics**: Token usage, latency, and error rates through Azure OpenAI
- Orchestrator supports concurrent agent execution

This Phase 1 implementation guide provides a comprehensive roadmap to establish the foundational multi-agent architecture while delivering immediate value through automated MITRE ATT&CK correlation and process analysis capabilities.